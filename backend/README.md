# Hackathon AI åŠ©æ‰‹åç«¯

åŸºäº Ollama çš„æœ¬åœ° AI åŠ©æ‰‹æœåŠ¡ï¼Œä¸º Hackathon å»ä¸­å¿ƒåŒ–äº¤æ˜“å¹³å°æä¾›æ™ºèƒ½å®¢æœåŠŸèƒ½ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¤– **æœ¬åœ°æ¨ç†**ï¼šä½¿ç”¨ Ollama åœ¨æœ¬åœ°è¿è¡Œ AI æ¨¡å‹ï¼Œä¿æŠ¤éšç§
- ğŸš€ **å¿«é€Ÿå“åº”**ï¼šé€šå¸¸ 5-15 ç§’å†…å®Œæˆå›ç­”
- ğŸ’° **é›¶æˆæœ¬**ï¼šæ— éœ€ API å¯†é’¥ï¼Œå®Œå…¨å…è´¹
- ğŸŒ **OpenAI å…¼å®¹**ï¼šæä¾› OpenAI æ ¼å¼çš„ API æ¥å£
- ğŸ”§ **æ˜“äºé…ç½®**ï¼šæ”¯æŒå¤šç§å¼€æºæ¨¡å‹

## ğŸ“‹ å‰ç½®è¦æ±‚

### 1. å®‰è£… Python
- Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- pip åŒ…ç®¡ç†å™¨

### 2. å®‰è£… Ollama

**Windows:**
```bash
# ä¸‹è½½å¹¶å®‰è£…
https://ollama.com/download/windows
```

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 3. æ‹‰å– AI æ¨¡å‹

æ¨èä½¿ç”¨ Qwen2.5ï¼ˆé»˜è®¤ï¼‰ï¼š
```bash
ollama pull qwen2.5:7b
```

å…¶ä»–å¯é€‰æ¨¡å‹ï¼š
```bash
# DeepSeek Coderï¼ˆé€‚åˆä»£ç ç›¸å…³é—®é¢˜ï¼‰
ollama pull deepseek-coder:6.7b

# Llama 3.1ï¼ˆé€šç”¨å¯¹è¯ï¼‰
ollama pull llama3.1:8b

# Mistralï¼ˆè½»é‡çº§ï¼‰
ollama pull mistral:7b
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒæ£€æŸ¥ï¼ˆæ¨èï¼‰

åœ¨å¯åŠ¨æœåŠ¡å™¨ä¹‹å‰ï¼Œå»ºè®®å…ˆè¿è¡Œç¯å¢ƒæ£€æŸ¥è„šæœ¬ï¼š

```bash
cd Hackathon/backend
python check_setup.py
```

è¿™ä¼šæ£€æŸ¥ï¼š
- âœ… Python ç‰ˆæœ¬
- âœ… Ollama å®‰è£…å’ŒæœåŠ¡çŠ¶æ€
- âœ… å·²å®‰è£…çš„æ¨¡å‹
- âœ… Python ä¾èµ–
- âœ… ç«¯å£å¯ç”¨æ€§

### Windows

1. **å¯åŠ¨ Ollama æœåŠ¡**ï¼ˆå¦‚æœæœªè‡ªåŠ¨å¯åŠ¨ï¼‰
```bash
ollama serve
```

2. **è¿è¡Œå¯åŠ¨è„šæœ¬**
```bash
cd Hackathon/backend
start_server.bat
```

### macOS/Linux

1. **å¯åŠ¨ Ollama æœåŠ¡**
```bash
ollama serve
```

2. **è¿è¡Œå¯åŠ¨è„šæœ¬**
```bash
cd Hackathon/backend
chmod +x start_server.sh
./start_server.sh
```

### æ‰‹åŠ¨å¯åŠ¨

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# 2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. å¯åŠ¨æœåŠ¡å™¨
python ai_assistant_server.py
```

## ğŸ”§ é…ç½®

### ç¯å¢ƒå˜é‡

å¤åˆ¶ `.env.example` ä¸º `.env` å¹¶ä¿®æ”¹é…ç½®ï¼š

```bash
cp .env.example .env
```

å¯é…ç½®é¡¹ï¼š
```env
# Ollama API åœ°å€
OLLAMA_API=http://localhost:11434/api/generate

# ä½¿ç”¨çš„æ¨¡å‹åç§°
MODEL_NAME=qwen2.5:7b

# æœåŠ¡å™¨é…ç½®
HOST=0.0.0.0
PORT=8000
```

### åˆ‡æ¢æ¨¡å‹

ä¿®æ”¹ `.env` æ–‡ä»¶ä¸­çš„ `MODEL_NAME`ï¼Œæˆ–åœ¨å¯åŠ¨æ—¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
# Windows
set MODEL_NAME=deepseek-coder:6.7b
python ai_assistant_server.py

# macOS/Linux
MODEL_NAME=deepseek-coder:6.7b python ai_assistant_server.py
```

## ğŸ“¡ API æ¥å£

### 1. å¥åº·æ£€æŸ¥
```http
GET http://localhost:8000/health
```

å“åº”ï¼š
```json
{
  "status": "ok",
  "engine": "Ollama",
  "model": "qwen2.5:7b",
  "ollama_status": "running",
  "timestamp": "2026-02-06T10:30:00"
}
```

### 2. AI åŠ©æ‰‹å¯¹è¯
```http
POST http://localhost:8000/v1/assistant/chat
Content-Type: application/json

{
  "messages": [
    {
      "role": "user",
      "content": "å¦‚ä½•è¿æ¥é’±åŒ…ï¼Ÿ"
    }
  ]
}
```

å“åº”ï¼š
```json
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "è¿æ¥é’±åŒ…çš„æ­¥éª¤å¦‚ä¸‹ï¼š\n1. ç‚¹å‡»é¡µé¢å³ä¸Šè§’..."
    },
    "finish_reason": "stop"
  }],
  "model": "qwen2.5:7b",
  "inference_time": 8.5
}
```

### 3. OpenAI å…¼å®¹æ¥å£
```http
POST http://localhost:8000/v1/chat/completions
Content-Type: application/json

{
  "messages": [
    {
      "role": "user",
      "content": "ä»€ä¹ˆæ˜¯å»ä¸­å¿ƒåŒ–äº¤æ˜“ï¼Ÿ"
    }
  ]
}
```

## ğŸ§ª æµ‹è¯•

### ä½¿ç”¨ curl æµ‹è¯•

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# AI å¯¹è¯
curl -X POST http://localhost:8000/v1/assistant/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "å¦‚ä½•è·å–æµ‹è¯•å¸ï¼Ÿ"}
    ]
  }'
```

### ä½¿ç”¨ Python æµ‹è¯•

```python
import requests

response = requests.post(
    "http://localhost:8000/v1/assistant/chat",
    json={
        "messages": [
            {"role": "user", "content": "äº¤æ˜“éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ"}
        ]
    }
)

print(response.json()["choices"][0]["message"]["content"])
```

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜ 1: Ollama æœåŠ¡æœªè¿è¡Œ
```
âŒ é”™è¯¯: æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å¯åŠ¨ Ollama
ollama serve

# æˆ–è€…åœ¨ Windows ä¸ŠåŒå‡» Ollama å›¾æ ‡
```

### é—®é¢˜ 2: æ¨¡å‹æœªæ‰¾åˆ°
```
âš ï¸ è­¦å‘Š: æ¨¡å‹ qwen2.5:7b æœªæ‰¾åˆ°
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ‹‰å–æ¨¡å‹
ollama pull qwen2.5:7b

# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list
```

### é—®é¢˜ 3: æ¨ç†è¶…æ—¶
```
âŒ æ¨ç†è¶…æ—¶ï¼ˆ120ç§’ï¼‰
```

**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ `mistral:7b`ï¼‰
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼ˆå»ºè®® 8GB+ï¼‰
- å¦‚æœæœ‰ GPUï¼Œç¡®ä¿ Ollama æ­£ç¡®ä½¿ç”¨ GPU

### é—®é¢˜ 4: ç«¯å£è¢«å ç”¨
```
âŒ Address already in use
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# ä¿®æ”¹ .env ä¸­çš„ PORT
PORT=8001

# æˆ–è€…æ‰¾åˆ°å¹¶å…³é—­å ç”¨ç«¯å£çš„è¿›ç¨‹
# Windows:
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# macOS/Linux:
lsof -i :8000
kill -9 <PID>
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. ä½¿ç”¨ GPU åŠ é€Ÿ

Ollama ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ GPUã€‚ç¡®ä¿å·²å®‰è£…ï¼š
- NVIDIA GPU: CUDA é©±åŠ¨
- AMD GPU: ROCm é©±åŠ¨
- Apple Silicon: è‡ªåŠ¨æ”¯æŒ

### 2. è°ƒæ•´æ¨¡å‹å¤§å°

æ ¹æ®ç¡¬ä»¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š
- **8GB RAM**: `mistral:7b`, `qwen2.5:7b`
- **16GB RAM**: `llama3.1:8b`, `deepseek-coder:6.7b`
- **32GB+ RAM**: æ›´å¤§çš„æ¨¡å‹

### 3. ä¼˜åŒ–æ¨ç†å‚æ•°

åœ¨ `ai_assistant_server.py` ä¸­è°ƒæ•´ï¼š
```python
"options": {
    "temperature": 0.7,      # é™ä½å¯æé«˜ä¸€è‡´æ€§
    "num_predict": 800,      # å‡å°‘å¯åŠ å¿«é€Ÿåº¦
    "top_p": 0.9,
    "top_k": 40
}
```

## ğŸ”— é›†æˆåˆ°å‰ç«¯

åœ¨å‰ç«¯é¡¹ç›®ä¸­è°ƒç”¨ AI åŠ©æ‰‹ï¼š

```typescript
// src/services/aiService.ts
export async function askAI(question: string) {
  const response = await fetch('http://localhost:8000/v1/assistant/chat', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      messages: [
        { role: 'user', content: question }
      ]
    })
  });
  
  const data = await response.json();
  return data.choices[0].message.content;
}
```

## ğŸ“š ç›¸å…³èµ„æº

- [Ollama å®˜ç½‘](https://ollama.com/)
- [Ollama æ¨¡å‹åº“](https://ollama.com/library)
- [Flask æ–‡æ¡£](https://flask.palletsprojects.com/)
- [Hackathon é¡¹ç›®æ–‡æ¡£](../README.md)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

---

**æ³¨æ„**ï¼šæœ¬æœåŠ¡ä»…ç”¨äºå¼€å‘å’Œæµ‹è¯•ç¯å¢ƒã€‚ç”Ÿäº§ç¯å¢ƒè¯·è€ƒè™‘ä½¿ç”¨ä¸“ä¸šçš„ AI æœåŠ¡ã€‚
