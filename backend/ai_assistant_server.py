"""
Hackathon AI åŠ©æ‰‹æœåŠ¡å™¨ - ä½¿ç”¨ Ollama æœ¬åœ°æ¨ç†
åŸºäº coconut-RustSentinel çš„å®ç°ï¼Œé€‚é… Hackathon é¡¹ç›®
"""
from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
import time
import os
from datetime import datetime

app = Flask(__name__)
CORS(app)

# Ollama API é…ç½®
OLLAMA_API = os.getenv("OLLAMA_API", "http://localhost:11434/api/generate")
MODEL_NAME = os.getenv("MODEL_NAME", "qwen2.5:7b")

print("=" * 60)
print("Hackathon AI åŠ©æ‰‹æœåŠ¡å™¨")
print(f"æ¨ç†å¼•æ“: Ollama")
print(f"æ¨¡å‹: {MODEL_NAME}")
print(f"API: {OLLAMA_API}")
print("=" * 60)

@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    # æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
    try:
        response = requests.get("http://localhost:11434", timeout=2)
        ollama_status = "running" if response.status_code == 200 else "error"
    except:
        ollama_status = "not running"
    
    return jsonify({
        "status": "ok",
        "engine": "Ollama",
        "model": MODEL_NAME,
        "ollama_status": ollama_status,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/v1/assistant/chat', methods=['POST'])
def assistant_chat():
    """
    AI åŠ©æ‰‹å¯¹è¯æ¥å£
    ç”¨äºå›ç­”ç”¨æˆ·å…³äº Hackathon å»ä¸­å¿ƒåŒ–äº¤æ˜“å¹³å°çš„é—®é¢˜
    """
    data = request.json
    messages = data.get('messages', [])
    
    if not messages:
        return jsonify({"error": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"}), 400
    
    # æ„å»º Hackathon å¹³å°ä¸“ç”¨æç¤ºè¯
    system_prompt = """ä½ æ˜¯ Hackathon å»ä¸­å¿ƒåŒ–äº¤æ˜“å¹³å°çš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ã€‚è¯·ç”¨ç®€æ´ã€å‹å¥½ã€ä¸“ä¸šçš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

# Hackathon å¹³å°ä»‹ç»
Hackathon æ˜¯ä¸€ä¸ªåŸºäº Conflux åŒºå—é“¾çš„å»ä¸­å¿ƒåŒ–äº¤æ˜“å¹³å°ï¼Œæ”¯æŒå¤šé’±åŒ…è¿æ¥å’Œå®‰å…¨çš„ç‚¹å¯¹ç‚¹äº¤æ˜“ã€‚

# ä¸»è¦åŠŸèƒ½
1. **å¤šé’±åŒ…æ”¯æŒ**
   - MetaMaskï¼ˆç‹ç‹¸é’±åŒ…ï¼‰- EVM å…¼å®¹
   - OKX Wallet - EVM å…¼å®¹  
   - Fluent Wallet - Conflux åŸç”Ÿé’±åŒ…

2. **å¤šç½‘ç»œæ”¯æŒ**
   - Conflux eSpace Testnetï¼ˆç”¨äº MetaMask/OKXï¼‰
   - Conflux Core Testnetï¼ˆç”¨äº Fluentï¼‰

3. **æ ¸å¿ƒäº¤æ˜“åŠŸèƒ½**
   - åˆ›å»ºå»ä¸­å¿ƒåŒ–äº¤æ˜“
   - æ¥å—å’Œç®¡ç†äº¤æ˜“
   - è‡ªåŠ¨æ‰˜ç®¡å’Œé‡Šæ”¾èµ„é‡‘
   - äº¤æ˜“çŠ¶æ€å®æ—¶è¿½è¸ª

4. **å¤šè¯­è¨€æ”¯æŒ**
   - ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰
   - Englishï¼ˆè‹±æ–‡ï¼‰
   - ç¹é«”ä¸­æ–‡ï¼ˆç¹ä½“ä¸­æ–‡ï¼‰

# ä½¿ç”¨æµç¨‹ï¼ˆè¯¦ç»†æ­¥éª¤ï¼‰

## 1. è¿æ¥é’±åŒ…
- ç‚¹å‡»é¡µé¢å³ä¸Šè§’"Connect Wallet"æŒ‰é’®
- ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å·²å®‰è£…çš„é’±åŒ…
- é€‰æ‹©ä½ çš„é’±åŒ…ï¼ˆMetaMask/OKX/Fluentï¼‰
- åœ¨é’±åŒ…å¼¹çª—ä¸­æˆæƒè¿æ¥
- å¦‚æœç½‘ç»œä¸æ­£ç¡®ï¼Œç³»ç»Ÿä¼šæç¤ºåˆ‡æ¢åˆ°æµ‹è¯•ç½‘

## 2. åˆ›å»ºäº¤æ˜“
- å¡«å†™äº¤æ˜“ä¿¡æ¯ï¼š
  * æ¥æ”¶æ–¹åœ°å€ï¼ˆå¯¹æ–¹é’±åŒ…åœ°å€ï¼‰
  * ä»£å¸ç±»å‹ï¼ˆCFX æˆ–å…¶ä»–æ”¯æŒçš„ä»£å¸ï¼‰
  * äº¤æ˜“é‡‘é¢
  * äº¤æ˜“æè¿°ï¼ˆå¯é€‰ï¼‰
- ç‚¹å‡»"åˆ›å»ºäº¤æ˜“"æŒ‰é’®
- åœ¨é’±åŒ…ä¸­ç¡®è®¤äº¤æ˜“
- ç­‰å¾…åŒºå—é“¾ç¡®è®¤ï¼ˆé€šå¸¸ 3-10 ç§’ï¼‰

## 3. æ¥å—äº¤æ˜“
- ä½œä¸ºæ¥æ”¶æ–¹ï¼Œåœ¨äº¤æ˜“åˆ—è¡¨ä¸­æ‰¾åˆ°å¾…æ¥å—çš„äº¤æ˜“
- ç‚¹å‡»"æ¥å—äº¤æ˜“"æŒ‰é’®
- åœ¨é’±åŒ…ä¸­ç¡®è®¤
- èµ„é‡‘ä¼šè¢«æ™ºèƒ½åˆçº¦æ‰˜ç®¡

## 4. å®Œæˆäº¤æ˜“
- åŒæ–¹ç¡®è®¤äº¤æ˜“å®Œæˆå
- ç‚¹å‡»"å®Œæˆäº¤æ˜“"æŒ‰é’®
- èµ„é‡‘ä¼šè‡ªåŠ¨ä»æ‰˜ç®¡é‡Šæ”¾åˆ°æ¥æ”¶æ–¹

## 5. å–æ¶ˆäº¤æ˜“
- åªæœ‰åˆ›å»ºè€…å¯ä»¥å–æ¶ˆæœªæ¥å—çš„äº¤æ˜“
- ç‚¹å‡»"å–æ¶ˆäº¤æ˜“"æŒ‰é’®
- èµ„é‡‘ä¼šé€€å›åˆ°åˆ›å»ºè€…è´¦æˆ·

# æ”¯æŒçš„ç½‘ç»œè¯¦æƒ…

## Conflux eSpace Testnetï¼ˆç”¨äº MetaMask/OKXï¼‰
- Chain ID: 71
- RPC URL: https://evmtestnet.confluxrpc.com
- åŒºå—æµè§ˆå™¨: https://evmtestnet.confluxscan.io
- æµ‹è¯•å¸æ°´é¾™å¤´: https://efaucet.confluxnetwork.org/

## Conflux Core Testnetï¼ˆç”¨äº Fluentï¼‰
- Network ID: 1
- RPC URL: https://test.confluxrpc.com
- åŒºå—æµè§ˆå™¨: https://testnet.confluxscan.io
- æµ‹è¯•å¸æ°´é¾™å¤´: https://faucet.confluxnetwork.org/

# å¸¸è§é—®é¢˜è§£ç­”

**Q: å¦‚ä½•è·å–æµ‹è¯•å¸ï¼Ÿ**
A: è®¿é—®å¯¹åº”ç½‘ç»œçš„æ°´é¾™å¤´ç½‘ç«™ï¼Œè¾“å…¥ä½ çš„é’±åŒ…åœ°å€å³å¯å…è´¹é¢†å–æµ‹è¯•å¸ã€‚eSpace ç”¨æˆ·è®¿é—® https://efaucet.confluxnetwork.org/ï¼ŒCore ç”¨æˆ·è®¿é—® https://faucet.confluxnetwork.org/

**Q: äº¤æ˜“éœ€è¦å¤šé•¿æ—¶é—´ç¡®è®¤ï¼Ÿ**
A: Conflux ç½‘ç»œç¡®è®¤é€Ÿåº¦å¾ˆå¿«ï¼Œé€šå¸¸ 3-10 ç§’å³å¯å®Œæˆä¸€ç¬”äº¤æ˜“ã€‚

**Q: äº¤æ˜“è´¹ç”¨æ˜¯å¤šå°‘ï¼Ÿ**
A: åœ¨æµ‹è¯•ç½‘ä¸Šï¼Œäº¤æ˜“è´¹ç”¨ï¼ˆGas Feeï¼‰éå¸¸ä½ï¼Œé€šå¸¸ä¸åˆ° 0.001 CFXã€‚æµ‹è¯•å¸å¯ä»¥å…è´¹ä»æ°´é¾™å¤´è·å–ã€‚

**Q: æ”¯æŒå“ªäº›ä»£å¸ï¼Ÿ**
A: ç›®å‰ä¸»è¦æ”¯æŒ CFXï¼ˆConflux åŸç”Ÿä»£å¸ï¼‰ã€‚æœªæ¥ä¼šæ”¯æŒæ›´å¤š ERC-20 ä»£å¸ã€‚

**Q: èµ„é‡‘å®‰å…¨å—ï¼Ÿ**
A: æ‰€æœ‰èµ„é‡‘ç”±æ™ºèƒ½åˆçº¦æ‰˜ç®¡ï¼Œåªæœ‰åœ¨åŒæ–¹ç¡®è®¤åæ‰ä¼šé‡Šæ”¾ã€‚ä½ çš„ç§é’¥å§‹ç»ˆç”±ä½ è‡ªå·±æ§åˆ¶ï¼Œå¹³å°æ— æ³•è®¿é—®ã€‚

**Q: å¯ä»¥åœ¨ä¸»ç½‘ä½¿ç”¨å—ï¼Ÿ**
A: ç›®å‰ä»…æ”¯æŒæµ‹è¯•ç½‘ã€‚è¯·å‹¿åœ¨ä¸»ç½‘ä½¿ç”¨çœŸå®èµ„äº§ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºé¡¹ç›®ã€‚

**Q: å¦‚ä½•åˆ‡æ¢è¯­è¨€ï¼Ÿ**
A: ç‚¹å‡»é¡µé¢å³ä¸Šè§’çš„è¯­è¨€åˆ‡æ¢æŒ‰é’®ï¼Œå¯ä»¥åœ¨ä¸­æ–‡ã€è‹±æ–‡ã€ç¹ä½“ä¸­æ–‡ä¹‹é—´åˆ‡æ¢ã€‚

**Q: é’±åŒ…è¿æ¥å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**
A: 
1. ç¡®ä¿å·²å®‰è£…å¯¹åº”çš„é’±åŒ…æ‰©å±•
2. æ£€æŸ¥é’±åŒ…æ˜¯å¦å·²è§£é”
3. å°è¯•åˆ·æ–°é¡µé¢é‡æ–°è¿æ¥
4. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸

**Q: äº¤æ˜“å¡ä½äº†æ€ä¹ˆåŠï¼Ÿ**
A: 
1. æ£€æŸ¥åŒºå—æµè§ˆå™¨ç¡®è®¤äº¤æ˜“çŠ¶æ€
2. å¦‚æœäº¤æ˜“å¤±è´¥ï¼Œå¯ä»¥é‡è¯•
3. ç¡®ä¿é’±åŒ…ä¸­æœ‰è¶³å¤Ÿçš„ Gas Fee

# æŠ€æœ¯æ ˆ
- å‰ç«¯: React 19 + TypeScript + Vite
- åŒºå—é“¾: Conflux eSpace + Conflux Core
- é’±åŒ…è¿æ¥: Wagmi (EVM) + js-conflux-sdk (Core)
- æ™ºèƒ½åˆçº¦: Solidity

# å›ç­”è§„åˆ™
1. ç›´æ¥ã€å‡†ç¡®åœ°å›ç­”é—®é¢˜ï¼Œç»™å‡ºå…·ä½“æ­¥éª¤å’Œæ•°å­—
2. å¦‚æœé—®é¢˜æ¶‰åŠæ“ä½œæ­¥éª¤ï¼ŒæŒ‰é¡ºåºåˆ—å‡º
3. æä¾›ç›¸å…³çš„é“¾æ¥å’Œèµ„æº
4. ä¿æŒå‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”
5. æ¯ä¸ªå›ç­”æ§åˆ¶åœ¨ 150-300 å­—ä»¥å†…
6. å¦‚æœé—®é¢˜è¶…å‡ºèŒƒå›´ï¼Œå»ºè®®ç”¨æˆ·æŸ¥çœ‹æ–‡æ¡£æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ"""
    
    # æ„å»ºå®Œæ•´çš„å¯¹è¯æç¤ºè¯
    prompt = f"System: {system_prompt}\n\n"
    
    for msg in messages:
        role = msg.get('role', '')
        content = msg.get('content', '')
        if role == 'user':
            prompt += f"User: {content}\n\n"
        elif role == 'assistant':
            prompt += f"Assistant: {content}\n\n"
    
    prompt += "è¯·ç”¨ä¸­æ–‡ç®€æ´ã€ä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n\nAssistant: "
    
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] æ”¶åˆ° AI åŠ©æ‰‹è¯·æ±‚")
    print(f"é—®é¢˜é•¿åº¦: {len(messages[-1].get('content', ''))} å­—ç¬¦")
    print("å¼€å§‹æ¨ç†...")
    
    start_time = time.time()
    
    try:
        # è°ƒç”¨ Ollama API
        response = requests.post(
            OLLAMA_API,
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,      # å¯¹è¯ä½¿ç”¨é€‚ä¸­çš„æ¸©åº¦
                    "num_predict": 800,      # é™åˆ¶å›ç­”é•¿åº¦
                    "top_p": 0.9,
                    "top_k": 40
                }
            },
            timeout=120  # 2 åˆ†é’Ÿè¶…æ—¶
        )
        
        inference_time = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            assistant_response = result.get('response', '').strip()
            
            print(f"âœ… æ¨ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.2f} ç§’")
            print(f"å“åº”é•¿åº¦: {len(assistant_response)} å­—ç¬¦")
            
            return jsonify({
                "choices": [{
                    "message": {
                        "role": "assistant",
                        "content": assistant_response
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": len(prompt),
                    "completion_tokens": len(assistant_response),
                    "total_tokens": len(prompt) + len(assistant_response)
                },
                "model": MODEL_NAME,
                "inference_time": round(inference_time, 2)
            })
        else:
            error_msg = f"Ollama API é”™è¯¯: {response.status_code}"
            print(f"âŒ {error_msg}")
            return jsonify({"error": error_msg}), 500
            
    except requests.exceptions.Timeout:
        error_msg = "æ¨ç†è¶…æ—¶ï¼ˆ120ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•"
        print(f"âŒ {error_msg}")
        return jsonify({"error": error_msg}), 504
    except requests.exceptions.ConnectionError:
        error_msg = "æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ï¼Œè¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ"
        print(f"âŒ {error_msg}")
        return jsonify({"error": error_msg}), 503
    except Exception as e:
        error_msg = f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        return jsonify({"error": error_msg}), 500

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """
    OpenAI å…¼å®¹çš„èŠå¤©æ¥å£
    å¯ä»¥ç”¨äºå…¶ä»–éœ€è¦ OpenAI API æ ¼å¼çš„åœºæ™¯
    """
    return assistant_chat()

if __name__ == '__main__':
    print("\n" + "="*60)
    print("ğŸš€ Hackathon AI åŠ©æ‰‹æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼")
    print("="*60)
    print(f"ğŸ“¡ API åœ°å€: http://localhost:8000")
    print(f"ğŸ”— å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    print(f"ğŸ’¬ å¯¹è¯æ¥å£: http://localhost:8000/v1/assistant/chat")
    print("="*60)
    
    # æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
    print("\nğŸ” æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€...")
    try:
        response = requests.get("http://localhost:11434", timeout=2)
        if response.status_code == 200:
            print("âœ… Ollama æœåŠ¡æ­£å¸¸è¿è¡Œ")
            
            # å°è¯•è·å–æ¨¡å‹åˆ—è¡¨
            try:
                models_response = requests.get("http://localhost:11434/api/tags", timeout=2)
                if models_response.status_code == 200:
                    models = models_response.json().get('models', [])
                    model_names = [m.get('name', '') for m in models]
                    
                    if MODEL_NAME in model_names:
                        print(f"âœ… æ¨¡å‹ {MODEL_NAME} å·²å°±ç»ª")
                    else:
                        print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹ {MODEL_NAME} æœªæ‰¾åˆ°")
                        print(f"   å¯ç”¨æ¨¡å‹: {', '.join(model_names) if model_names else 'æ— '}")
                        print(f"   è¯·è¿è¡Œ: ollama pull {MODEL_NAME}")
            except:
                pass
        else:
            print("âš ï¸  è­¦å‘Š: Ollama æœåŠ¡å“åº”å¼‚å¸¸")
    except:
        print("âŒ é”™è¯¯: æ— æ³•è¿æ¥åˆ° Ollama")
        print("   è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ")
        print("   å¯åŠ¨å‘½ä»¤: ollama serve")
        print(f"   æ‹‰å–æ¨¡å‹: ollama pull {MODEL_NAME}")
    
    print("\n" + "="*60)
    print("æœåŠ¡å™¨è¿è¡Œä¸­... æŒ‰ Ctrl+C åœæ­¢")
    print("="*60 + "\n")
    
    app.run(host='0.0.0.0', port=8000, debug=False)
