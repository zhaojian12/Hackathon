"""
Ollama è¿æ¥è¯Šæ–­è„šæœ¬
å¿«é€Ÿè¯Šæ–­ Ollama API 404 é”™è¯¯
"""
import requests
import json
import os
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

print("=" * 60)
print("Ollama è¿æ¥è¯Šæ–­")
print("=" * 60)

# 1. æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ
print("\n1ï¸âƒ£ æ£€æŸ¥ Ollama æœåŠ¡...")
try:
    response = requests.get("http://localhost:11434", timeout=2)
    if response.status_code == 200:
        print("   âœ… Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ")
    else:
        print(f"   âš ï¸  Ollama æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
except Exception as e:
    print(f"   âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡")
    print(f"   é”™è¯¯: {e}")
    print("\n   è§£å†³æ–¹æ¡ˆ:")
    print("   1. å¯åŠ¨ Ollama: ollama serve")
    print("   2. æˆ–è€…åœ¨ Windows ä¸ŠåŒå‡» Ollama å›¾æ ‡")
    exit(1)

# 2. æ£€æŸ¥å·²å®‰è£…çš„æ¨¡å‹
print("\n2ï¸âƒ£ æ£€æŸ¥å·²å®‰è£…çš„æ¨¡å‹...")
try:
    response = requests.get("http://localhost:11434/api/tags", timeout=5)
    if response.status_code == 200:
        data = response.json()
        models = data.get('models', [])
        
        if models:
            print(f"   âœ… æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹:")
            for model in models:
                name = model.get('name', 'unknown')
                size = model.get('size', 0) / (1024**3)  # è½¬æ¢ä¸º GB
                print(f"      - {name} ({size:.2f} GB)")
        else:
            print("   âš ï¸  æœªå®‰è£…ä»»ä½•æ¨¡å‹")
            print("\n   è§£å†³æ–¹æ¡ˆ:")
            print("   ollama pull qwen2.5:7b")
    else:
        print(f"   âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code}")
except Exception as e:
    print(f"   âŒ æ£€æŸ¥æ¨¡å‹å¤±è´¥: {e}")

# 3. æµ‹è¯• generate API
print("\n3ï¸âƒ£ æµ‹è¯• Ollama generate API...")
# ä»ç¯å¢ƒå˜é‡è¯»å–æ¨¡å‹åç§°
test_model = os.getenv("MODEL_NAME", "qwen3:4b-instruct-2507-q4_K_M")

try:
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": test_model,
            "prompt": "ä½ å¥½",
            "stream": False
        },
        timeout=30
    )
    
    if response.status_code == 200:
        print(f"   âœ… API æµ‹è¯•æˆåŠŸ")
        result = response.json()
        print(f"   å“åº”: {result.get('response', '')[:50]}...")
    elif response.status_code == 404:
        print(f"   âŒ 404 é”™è¯¯ - æ¨¡å‹ '{test_model}' æœªæ‰¾åˆ°")
        print("\n   è§£å†³æ–¹æ¡ˆ:")
        print(f"   ollama pull {test_model}")
        print("\n   æˆ–è€…ä½¿ç”¨å…¶ä»–å·²å®‰è£…çš„æ¨¡å‹ï¼Œä¿®æ”¹ .env æ–‡ä»¶:")
        print("   MODEL_NAME=ä½ çš„æ¨¡å‹åç§°")
    else:
        print(f"   âŒ API æµ‹è¯•å¤±è´¥: {response.status_code}")
        print(f"   å“åº”: {response.text}")
        
except requests.exceptions.Timeout:
    print(f"   âš ï¸  è¯·æ±‚è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
    print("   æ¨¡å‹å¯èƒ½æ­£åœ¨åŠ è½½ï¼Œè¯·ç¨åå†è¯•")
except Exception as e:
    print(f"   âŒ API æµ‹è¯•å¤±è´¥: {e}")

# 4. æ£€æŸ¥ç¯å¢ƒå˜é‡
print("\n4ï¸âƒ£ æ£€æŸ¥ç¯å¢ƒå˜é‡...")
ollama_api = os.getenv("OLLAMA_API", "http://localhost:11434/api/generate")
model_name = os.getenv("MODEL_NAME", "qwen3:4b-instruct-2507-q4_K_M")

print(f"   OLLAMA_API: {ollama_api}")
print(f"   MODEL_NAME: {model_name}")

# æ£€æŸ¥ .env æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if os.path.exists(".env"):
    print(f"   âœ… .env æ–‡ä»¶å­˜åœ¨")
else:
    print(f"   âš ï¸  .env æ–‡ä»¶ä¸å­˜åœ¨")

# 5. æä¾›è§£å†³æ–¹æ¡ˆ
print("\n" + "=" * 60)
print("è¯Šæ–­å®Œæˆ")
print("=" * 60)

print("\nğŸ“‹ å¸¸è§è§£å†³æ–¹æ¡ˆ:")
print("\n1. å¦‚æœ Ollama æœåŠ¡æœªè¿è¡Œ:")
print("   ollama serve")

print("\n2. å¦‚æœæ¨¡å‹æœªå®‰è£…:")
print("   ollama pull qwen2.5:7b")
print("   æˆ–è€…:")
print("   ollama pull deepseek-coder:6.7b")
print("   ollama pull llama3.1:8b")

print("\n3. å¦‚æœæƒ³ä½¿ç”¨å…¶ä»–æ¨¡å‹:")
print("   åˆ›å»º .env æ–‡ä»¶:")
print("   MODEL_NAME=ä½ çš„æ¨¡å‹åç§°")

print("\n4. æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹:")
print("   ollama list")

print("\n5. æµ‹è¯• Ollama:")
print("   ollama run qwen2.5:7b")
print("   ç„¶åè¾“å…¥: ä½ å¥½")

print("\n" + "=" * 60)
