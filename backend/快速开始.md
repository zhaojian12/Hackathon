# Hackathon AI åŠ©æ‰‹ - å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“¦ ä¸€é”®å¯åŠ¨ï¼ˆæ¨èï¼‰

### Windows ç”¨æˆ·

```bash
cd Hackathon\backend
start_server.bat
```

### macOS/Linux ç”¨æˆ·

```bash
cd Hackathon/backend
chmod +x start_server.sh
./start_server.sh
```

è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- âœ… æ£€æŸ¥ Python ç¯å¢ƒ
- âœ… åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
- âœ… å®‰è£…ä¾èµ–
- âœ… æ£€æŸ¥ Ollama æœåŠ¡
- âœ… å¯åŠ¨ AI åŠ©æ‰‹æœåŠ¡å™¨

## ğŸ” ç¯å¢ƒæ£€æŸ¥ï¼ˆæ¨èï¼‰

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¿è¡Œç¯å¢ƒæ£€æŸ¥è„šæœ¬ï¼š

```bash
cd Hackathon/backend
python check_setup.py
```

è¿™ä¼šè‡ªåŠ¨æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„ä¾èµ–å’Œé…ç½®ã€‚

## ğŸ”§ é¦–æ¬¡ä½¿ç”¨é…ç½®

### 1. å®‰è£… Ollama

**Windows:**
è®¿é—® https://ollama.com/download/windows ä¸‹è½½å®‰è£…

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. å¯åŠ¨ Ollama æœåŠ¡

```bash
ollama serve
```

æˆ–è€…åœ¨ Windows ä¸ŠåŒå‡» Ollama å›¾æ ‡å¯åŠ¨ã€‚

### 3. ä¸‹è½½ AI æ¨¡å‹

```bash
# æ¨èï¼šQwen2.5ï¼ˆä¸­æ–‡å‹å¥½ï¼Œé€Ÿåº¦å¿«ï¼‰
ollama pull qwen2.5:7b

# æˆ–è€…ï¼šDeepSeek Coderï¼ˆä»£ç ç›¸å…³é—®é¢˜æ›´å¥½ï¼‰
ollama pull deepseek-coder:6.7b
```

### 4. å¯åŠ¨ AI åŠ©æ‰‹æœåŠ¡å™¨

è¿è¡Œå¯åŠ¨è„šæœ¬ï¼ˆè§ä¸Šæ–¹ï¼‰æˆ–æ‰‹åŠ¨å¯åŠ¨ï¼š

```bash
cd Hackathon/backend
python ai_assistant_server.py
```

## âœ… éªŒè¯å®‰è£…

### 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€

è®¿é—®ï¼šhttp://localhost:8000/health

åº”è¯¥çœ‹åˆ°ï¼š
```json
{
  "status": "ok",
  "engine": "Ollama",
  "model": "qwen2.5:7b",
  "ollama_status": "running"
}
```

### 2. æµ‹è¯• AI å¯¹è¯

```bash
curl -X POST http://localhost:8000/v1/assistant/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"ä½ å¥½"}]}'
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### å¸¸è§é—®é¢˜

**Q: å¦‚ä½•è¿æ¥é’±åŒ…ï¼Ÿ**
```bash
curl -X POST http://localhost:8000/v1/assistant/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"å¦‚ä½•è¿æ¥é’±åŒ…ï¼Ÿ"}]}'
```

**Q: å¦‚ä½•è·å–æµ‹è¯•å¸ï¼Ÿ**
```bash
curl -X POST http://localhost:8000/v1/assistant/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"å¦‚ä½•è·å–æµ‹è¯•å¸ï¼Ÿ"}]}'
```

**Q: äº¤æ˜“éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ**
```bash
curl -X POST http://localhost:8000/v1/assistant/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"äº¤æ˜“éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ"}]}'
```

## ğŸ” å¸¸è§é—®é¢˜

### Ollama æœåŠ¡æœªè¿è¡Œ

**ç—‡çŠ¶ï¼š**
```
âŒ é”™è¯¯: æ— æ³•è¿æ¥åˆ° Ollama
```

**è§£å†³ï¼š**
```bash
# å¯åŠ¨ Ollama
ollama serve
```

### æ¨¡å‹æœªæ‰¾åˆ°

**ç—‡çŠ¶ï¼š**
```
âš ï¸ è­¦å‘Š: æ¨¡å‹ qwen2.5:7b æœªæ‰¾åˆ°
```

**è§£å†³ï¼š**
```bash
# æ‹‰å–æ¨¡å‹
ollama pull qwen2.5:7b

# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list
```

### ç«¯å£è¢«å ç”¨

**ç—‡çŠ¶ï¼š**
```
âŒ Address already in use
```

**è§£å†³ï¼š**
ä¿®æ”¹ `.env` æ–‡ä»¶ä¸­çš„ç«¯å£ï¼š
```env
PORT=8001
```

### æ¨ç†é€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆï¼š**
1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š`ollama pull mistral:7b`
2. ç¡®ä¿æœ‰è¶³å¤Ÿå†…å­˜ï¼ˆå»ºè®® 8GB+ï¼‰
3. ä½¿ç”¨ GPU åŠ é€Ÿï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰

## ğŸ“Š ç³»ç»Ÿè¦æ±‚

### æœ€ä½é…ç½®
- CPU: 4 æ ¸å¿ƒ
- RAM: 8GB
- ç£ç›˜: 10GB å¯ç”¨ç©ºé—´
- ç½‘ç»œ: ç¨³å®šçš„äº’è”ç½‘è¿æ¥ï¼ˆé¦–æ¬¡ä¸‹è½½æ¨¡å‹ï¼‰

### æ¨èé…ç½®
- CPU: 8 æ ¸å¿ƒ
- RAM: 16GB
- GPU: NVIDIA/AMDï¼ˆå¯é€‰ï¼Œæ˜¾è‘—æå‡é€Ÿåº¦ï¼‰
- ç£ç›˜: 20GB SSD

## ğŸš€ ä¸‹ä¸€æ­¥

1. âœ… å¯åŠ¨å‰ç«¯é¡¹ç›®ï¼ˆè§ `../frontend/README.md`ï¼‰
2. âœ… åœ¨å‰ç«¯é›†æˆ AI åŠ©æ‰‹ç»„ä»¶
3. âœ… æµ‹è¯•å®Œæ•´çš„ç”¨æˆ·æµç¨‹

## ğŸ“š æ›´å¤šæ–‡æ¡£

- [å®Œæ•´ README](./README.md) - è¯¦ç»†é…ç½®å’Œ API æ–‡æ¡£
- [å‰ç«¯é›†æˆæŒ‡å—](../frontend/AI_ASSISTANT_SETUP.md) - å¦‚ä½•åœ¨å‰ç«¯ä½¿ç”¨
- [Ollama æ–‡æ¡£](https://github.com/ollama/ollama) - Ollama å®˜æ–¹æ–‡æ¡£

## ğŸ’¡ æç¤º

- é¦–æ¬¡æ¨ç†ä¼šè¾ƒæ…¢ï¼ˆåŠ è½½æ¨¡å‹ï¼‰ï¼Œåç»­ä¼šå¿«å¾ˆå¤š
- æ¨¡å‹ä¼šåœ¨å†…å­˜ä¸­ä¿ç•™ 5 åˆ†é’Ÿï¼Œé¢‘ç¹ä½¿ç”¨æ—¶é€Ÿåº¦æ›´å¿«
- å¯ä»¥åŒæ—¶è¿è¡Œå¤šä¸ªæ¨¡å‹ï¼Œä½†ä¼šå ç”¨æ›´å¤šå†…å­˜

---

**å¼€å‘å®Œæˆ âœ… | ç‰ˆæœ¬ v1.0.0 | 2026-02-06**
